{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WslhYGMeiR6X"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"unit1.txt\""
      ],
      "metadata": {
        "id": "zGqAxm9Sj1B_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "print(\"File loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-P1g-sSj-aj",
        "outputId": "513565a4-8c82-4a6e-aff5-c697a4bec98e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:500] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts9aG509kJra",
        "outputId": "876b687f-a24a-436a-8780-8ede4f42fd3f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI and Its Applications: A Foundational Briefing\n",
            "\n",
            "Executive Summary\n",
            "\n",
            "This document provides a comprehensive overview of Generative AI, synthesizing foundational concepts, technological underpinnings, and practical applications as outlined in the course materials from PES University. Generative AI represents a transformative subset of Artificial Intelligence focused on creating novel content, a capability primarily driven by the advent of Large Language Models (LLMs). The evolution of ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)"
      ],
      "metadata": {
        "id": "kLUJN5WmkVnn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text_generation = \"The future of Artificial Intelligence is\"\n",
        "prompt_masked_bert = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "prompt_masked_roberta = \"The goal of Generative AI is to <mask> new content.\"\n",
        "prompt_qa = \"What are the risks?\"\n",
        "context_qa = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\""
      ],
      "metadata": {
        "id": "TfG1PALAkXGH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_text_generation = pipeline('text-generation', model='bert-base-uncased')\n",
        "roberta_text_generation = pipeline('text-generation', model='roberta-base')\n",
        "bart_text_generation = pipeline('text-generation', model='facebook/bart-base')\n",
        "\n",
        "bert_masked_lang = pipeline('fill-mask', model='bert-base-uncased')\n",
        "roberta_masked_lang = pipeline('fill-mask', model='roberta-base')\n",
        "bart_masked_lang = pipeline('fill-mask', model='facebook/bart-base')\n",
        "\n",
        "bert_qa = pipeline('question-answering', model='bert-base-uncased')\n",
        "roberta_qa = pipeline('question-answering', model='roberta-base')\n",
        "bart_qa = pipeline('question-answering', model='facebook/bart-base')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFmvcBsbk7CS",
        "outputId": "9154abe7-b8ec-4d47-e6e9-5a9da7650514"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cuda:0\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cuda:0\n",
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_bert_text_generation = bert_text_generation(prompt_text_generation, max_length=50, do_sample=True)\n",
        "output_roberta_text_generation = roberta_text_generation(prompt_text_generation, max_length=50, do_sample=True)\n",
        "output_bart_text_generation = bart_text_generation(prompt_text_generation, max_length=50, do_sample=True)\n",
        "\n",
        "output_bert_masked_lang = bert_masked_lang(prompt_masked_bert)\n",
        "output_roberta_masked_lang = roberta_masked_lang(prompt_masked_roberta)\n",
        "output_bart_masked_lang = bart_masked_lang(prompt_masked_roberta)\n",
        "\n",
        "output_bert_qa = bert_qa(question=prompt_qa, context=context_qa)\n",
        "output_roberta_qa = roberta_qa(question=prompt_qa, context=context_qa)\n",
        "output_bart_qa = bart_qa(question=prompt_qa, context=context_qa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i0vT1Y0mKnA",
        "outputId": "9c82d2dd-7277-4323-a6dd-f084e762256c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n===== TEXT GENERATION OUTPUTS =====\")\n",
        "\n",
        "try:\n",
        "    print(\"\\nBERT Text Generation:\")\n",
        "    print(output_bert_text_generation)\n",
        "except Exception as e:\n",
        "    print(\"BERT FAILED\", e)\n",
        "\n",
        "try:\n",
        "    print(\"\\nRoBERTa Text Generation:\")\n",
        "    print(output_roberta_text_generation)\n",
        "except Exception as e:\n",
        "    print(\"RoBERTa FAILED\", e)\n",
        "\n",
        "try:\n",
        "    print(\"\\nBART Text Generation:\")\n",
        "    print(output_bart_text_generation)\n",
        "except Exception as e:\n",
        "    print(\"BART FAILED\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjX_8K29nCnS",
        "outputId": "fcaded3c-c370-478d-ff4c-17a9f6eb0922"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== TEXT GENERATION OUTPUTS =====\n",
            "\n",
            "BERT Text Generation:\n",
            "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n",
            "\n",
            "RoBERTa Text Generation:\n",
            "[{'generated_text': 'The future of Artificial Intelligence is'}]\n",
            "\n",
            "BART Text Generation:\n",
            "[{'generated_text': 'The future of Artificial Intelligence is MAD hops hops hops Stuart � unim detectives detectives detectives condowolf Treasure optionally distinguishes optionally optionally boost Cosbyweb Dani Idol boost boostancesWindows cumbersome cumbersome charms charmsNPRances 560 560 boostORGE guruPrevious boostousands cumbersome cumbersome cumbersome � carbonacle Dani cumbersomeousands XIV optionally \\\\(\\\\ welcoming welcoming 421 charms Cosby welcoming charmsename cumbersome 560 boost \\\\(\\\\ XIV Idol optionally boost boostORGE Dani Dani Dani 560 carb boost cumbersome � 560 Dani APPLIC appendix � cumbersome Dani Toy Dani Dani *** cumbersome cumbersomemeasures boostORGE cumbersome shores Toy Ahmad cumbersomeename racket cumbersome cumbersome appendix Ahmad charms hearPT robots aggressively XIV cumbersome Users XIV WC cumbersomeORGE commentator Dani cumbersome cumbersome Dani Dani� XIV426�� Dani appendixsur cumbersome Dani�� pled aggressively� cumbersome� Daniename cumbersomeenameccess cumbersome426 cumbersome� XIV cumbersome cumbersome shores boost��� traditionally cumbersome cumbersomeweb cumbersome� appendixsur appendix aggressively Dani� boost�426 Dani appendix cumbersomeccess�� cumbersome Cosby Cosby predators Dani optionally�� appendix cumbersomeonding cumbersome� countryside Dani XIV Cosby gestation� Dani426� Dani Contribut cumbersome cumbersome XIVsur optionally Danisur cumbersome CosbyMaker cumbersome� cumbersome predators cumbersomecommon Dani XIVsur426 cumbersome Cosby installationsur cumbersomeORGEORGE Cannabissur�� optionally Dani Dani cumbersome documenting cumbersome XIV XIV XIV layoffs XIVsur stereo XIVcommonename Danisur Dani XIV boost cumbersome'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n===== FILL MASK OUTPUTS =====\")\n",
        "\n",
        "try:\n",
        "    print(\"\\nBERT Fill Mask:\")\n",
        "    print(output_bert_masked_lang)\n",
        "except Exception as e:\n",
        "    print(\"BERT FAILED\", e)\n",
        "\n",
        "try:\n",
        "    print(\"\\nRoBERTa Fill Mask:\")\n",
        "    print(output_roberta_masked_lang)\n",
        "except Exception as e:\n",
        "    print(\"RoBERTa FAILED\", e)\n",
        "\n",
        "try:\n",
        "    print(\"\\nBART Fill Mask:\")\n",
        "    print(output_bart_masked_lang)\n",
        "except Exception as e:\n",
        "    print(\"BART FAILED\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b-7eBsunnkA",
        "outputId": "b69eea28-fb68-44b0-8bc4-50b2a1a919c0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== FILL MASK OUTPUTS =====\n",
            "\n",
            "BERT Fill Mask:\n",
            "[{'score': 0.5396924614906311, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575772523880005, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}, {'score': 0.054054826498031616, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to produce new content.'}, {'score': 0.04451558738946915, 'token': 4503, 'token_str': 'develop', 'sequence': 'the goal of generative ai is to develop new content.'}, {'score': 0.017577439546585083, 'token': 5587, 'token_str': 'add', 'sequence': 'the goal of generative ai is to add new content.'}]\n",
            "\n",
            "RoBERTa Fill Mask:\n",
            "[{'score': 0.37113112211227417, 'token': 5368, 'token_str': ' generate', 'sequence': 'The goal of Generative AI is to generate new content.'}, {'score': 0.36771443486213684, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.08351481705904007, 'token': 8286, 'token_str': ' discover', 'sequence': 'The goal of Generative AI is to discover new content.'}, {'score': 0.021335195749998093, 'token': 465, 'token_str': ' find', 'sequence': 'The goal of Generative AI is to find new content.'}, {'score': 0.016521582379937172, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}]\n",
            "\n",
            "BART Fill Mask:\n",
            "[{'score': 0.07461563497781754, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.06571857631206512, 'token': 244, 'token_str': ' help', 'sequence': 'The goal of Generative AI is to help new content.'}, {'score': 0.06087999790906906, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}, {'score': 0.03593574836850166, 'token': 3155, 'token_str': ' enable', 'sequence': 'The goal of Generative AI is to enable new content.'}, {'score': 0.03319474682211876, 'token': 1477, 'token_str': ' improve', 'sequence': 'The goal of Generative AI is to improve new content.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n===== QUESTION ANSWERING OUTPUTS =====\")\n",
        "\n",
        "try:\n",
        "    print(\"\\nBERT QA:\")\n",
        "    print(output_bert_qa)\n",
        "except Exception as e:\n",
        "    print(\"BERT FAILED\", e)\n",
        "\n",
        "try:\n",
        "    print(\"\\nRoBERTa QA:\")\n",
        "    print(output_roberta_qa)\n",
        "except Exception as e:\n",
        "    print(\"RoBERTa FAILED\", e)\n",
        "\n",
        "try:\n",
        "    print(\"\\nBART QA:\")\n",
        "    print(output_bart_qa)\n",
        "except Exception as e:\n",
        "    print(\"BART FAILED\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiZG6_zhnplQ",
        "outputId": "830640ea-a706-4061-857b-6ea6e74f6ae1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== QUESTION ANSWERING OUTPUTS =====\n",
            "\n",
            "BERT QA:\n",
            "{'score': 0.008298376109451056, 'start': 66, 'end': 81, 'answer': ', and deepfakes'}\n",
            "\n",
            "RoBERTa QA:\n",
            "{'score': 0.007723246235400438, 'start': 0, 'end': 10, 'answer': 'Generative'}\n",
            "\n",
            "BART QA:\n",
            "{'score': 0.01929371803998947, 'start': 60, 'end': 61, 'answer': ','}\n"
          ]
        }
      ]
    }
  ]
}